{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, tensor\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import time \n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm_notebook\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_model(nn.Module) :\n",
    "    \"\"\"\n",
    "    model takes 2 frames as input uses same cnn on them \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cnn, cnn_out_sz, n_actions) :\n",
    "        super(Q_model, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.lin = nn.Linear(2*cnn_out_sz, n_actions)\n",
    "    \n",
    "    def forward(self, frame1, frame2) :\n",
    "        out1 = self.cnn(frame1)\n",
    "        out2 = self.cnn(frame2)\n",
    "        \n",
    "        linear_input = torch.cat([out1, out2], 1)\n",
    "        action_vals = self.lin(linear_input)\n",
    "        return action_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(210, 160, 3), Discrete(4))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6228404104894141ae97b0557de8c9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 246 timesteps, acc_reward 2.0\n",
      "Episode finished after 298 timesteps, acc_reward 2.0\n",
      "Episode finished after 305 timesteps, acc_reward 2.0\n",
      "Episode finished after 247 timesteps, acc_reward 1.0\n",
      "Episode finished after 203 timesteps, acc_reward 0.0\n",
      "Episode finished after 172 timesteps, acc_reward 0.0\n",
      "Episode finished after 196 timesteps, acc_reward 0.0\n",
      "Episode finished after 249 timesteps, acc_reward 2.0\n",
      "Episode finished after 255 timesteps, acc_reward 1.0\n",
      "Episode finished after 185 timesteps, acc_reward 0.0\n",
      "Episode finished after 181 timesteps, acc_reward 0.0\n",
      "Episode finished after 191 timesteps, acc_reward 0.0\n",
      "Episode finished after 327 timesteps, acc_reward 2.0\n",
      "Episode finished after 184 timesteps, acc_reward 0.0\n",
      "Episode finished after 209 timesteps, acc_reward 1.0\n",
      "Episode finished after 277 timesteps, acc_reward 2.0\n",
      "Episode finished after 186 timesteps, acc_reward 0.0\n",
      "Episode finished after 279 timesteps, acc_reward 1.0\n",
      "Episode finished after 235 timesteps, acc_reward 1.0\n",
      "Episode finished after 207 timesteps, acc_reward 1.0\n",
      "Episode finished after 338 timesteps, acc_reward 3.0\n",
      "Episode finished after 231 timesteps, acc_reward 1.0\n",
      "Episode finished after 181 timesteps, acc_reward 0.0\n",
      "Episode finished after 322 timesteps, acc_reward 3.0\n",
      "Episode finished after 308 timesteps, acc_reward 2.0\n",
      "Episode finished after 179 timesteps, acc_reward 0.0\n",
      "Episode finished after 402 timesteps, acc_reward 4.0\n",
      "Episode finished after 311 timesteps, acc_reward 2.0\n",
      "Episode finished after 189 timesteps, acc_reward 0.0\n",
      "Episode finished after 335 timesteps, acc_reward 3.0\n",
      "Episode finished after 165 timesteps, acc_reward 0.0\n",
      "Episode finished after 233 timesteps, acc_reward 0.0\n",
      "Episode finished after 180 timesteps, acc_reward 0.0\n",
      "Episode finished after 224 timesteps, acc_reward 1.0\n",
      "Episode finished after 164 timesteps, acc_reward 0.0\n",
      "Episode finished after 320 timesteps, acc_reward 2.0\n",
      "Episode finished after 295 timesteps, acc_reward 2.0\n",
      "Episode finished after 366 timesteps, acc_reward 3.0\n",
      "Episode finished after 247 timesteps, acc_reward 1.0\n",
      "Episode finished after 178 timesteps, acc_reward 0.0\n",
      "Episode finished after 179 timesteps, acc_reward 0.0\n",
      "Episode finished after 242 timesteps, acc_reward 1.0\n",
      "Episode finished after 467 timesteps, acc_reward 5.0\n",
      "Episode finished after 350 timesteps, acc_reward 3.0\n",
      "Episode finished after 238 timesteps, acc_reward 1.0\n",
      "Episode finished after 209 timesteps, acc_reward 1.0\n",
      "Episode finished after 287 timesteps, acc_reward 2.0\n",
      "Episode finished after 183 timesteps, acc_reward 0.0\n",
      "Episode finished after 412 timesteps, acc_reward 4.0\n",
      "Episode finished after 175 timesteps, acc_reward 0.0\n",
      "Episode finished after 222 timesteps, acc_reward 1.0\n",
      "Episode finished after 251 timesteps, acc_reward 1.0\n",
      "Episode finished after 184 timesteps, acc_reward 0.0\n",
      "Episode finished after 233 timesteps, acc_reward 1.0\n",
      "Episode finished after 237 timesteps, acc_reward 1.0\n",
      "Episode finished after 276 timesteps, acc_reward 1.0\n",
      "Episode finished after 265 timesteps, acc_reward 2.0\n",
      "Episode finished after 308 timesteps, acc_reward 2.0\n",
      "Episode finished after 178 timesteps, acc_reward 0.0\n",
      "Episode finished after 355 timesteps, acc_reward 3.0\n",
      "Episode finished after 189 timesteps, acc_reward 0.0\n",
      "Episode finished after 242 timesteps, acc_reward 1.0\n",
      "Episode finished after 171 timesteps, acc_reward 0.0\n",
      "Episode finished after 288 timesteps, acc_reward 2.0\n",
      "Episode finished after 375 timesteps, acc_reward 4.0\n",
      "Episode finished after 184 timesteps, acc_reward 0.0\n",
      "Episode finished after 267 timesteps, acc_reward 2.0\n",
      "Episode finished after 324 timesteps, acc_reward 3.0\n",
      "Episode finished after 440 timesteps, acc_reward 5.0\n",
      "Episode finished after 170 timesteps, acc_reward 0.0\n",
      "Episode finished after 172 timesteps, acc_reward 0.0\n",
      "Episode finished after 365 timesteps, acc_reward 3.0\n",
      "Episode finished after 212 timesteps, acc_reward 1.0\n",
      "Episode finished after 177 timesteps, acc_reward 0.0\n",
      "Episode finished after 305 timesteps, acc_reward 3.0\n",
      "Episode finished after 252 timesteps, acc_reward 2.0\n",
      "Episode finished after 233 timesteps, acc_reward 1.0\n",
      "Episode finished after 211 timesteps, acc_reward 1.0\n",
      "Episode finished after 240 timesteps, acc_reward 1.0\n",
      "Episode finished after 335 timesteps, acc_reward 3.0\n",
      "Episode finished after 407 timesteps, acc_reward 3.0\n",
      "Episode finished after 316 timesteps, acc_reward 2.0\n",
      "Episode finished after 301 timesteps, acc_reward 2.0\n",
      "Episode finished after 274 timesteps, acc_reward 2.0\n",
      "Episode finished after 181 timesteps, acc_reward 0.0\n",
      "Episode finished after 340 timesteps, acc_reward 3.0\n",
      "Episode finished after 307 timesteps, acc_reward 2.0\n",
      "Episode finished after 480 timesteps, acc_reward 5.0\n",
      "Episode finished after 238 timesteps, acc_reward 1.0\n",
      "Episode finished after 233 timesteps, acc_reward 1.0\n",
      "Episode finished after 285 timesteps, acc_reward 2.0\n",
      "Episode finished after 212 timesteps, acc_reward 0.0\n",
      "Episode finished after 178 timesteps, acc_reward 0.0\n",
      "Episode finished after 168 timesteps, acc_reward 0.0\n",
      "Episode finished after 179 timesteps, acc_reward 0.0\n",
      "Episode finished after 216 timesteps, acc_reward 1.0\n",
      "Episode finished after 302 timesteps, acc_reward 2.0\n",
      "Episode finished after 219 timesteps, acc_reward 1.0\n",
      "Episode finished after 229 timesteps, acc_reward 1.0\n",
      "Episode finished after 175 timesteps, acc_reward 0.0\n",
      "Episode finished after 247 timesteps, acc_reward 1.0\n",
      "Episode finished after 336 timesteps, acc_reward 3.0\n",
      "Episode finished after 269 timesteps, acc_reward 1.0\n",
      "Episode finished after 248 timesteps, acc_reward 1.0\n",
      "Episode finished after 197 timesteps, acc_reward 0.0\n",
      "Episode finished after 194 timesteps, acc_reward 0.0\n",
      "Episode finished after 191 timesteps, acc_reward 0.0\n",
      "Episode finished after 188 timesteps, acc_reward 0.0\n",
      "Episode finished after 172 timesteps, acc_reward 0.0\n",
      "Episode finished after 242 timesteps, acc_reward 1.0\n",
      "Episode finished after 253 timesteps, acc_reward 1.0\n",
      "Episode finished after 258 timesteps, acc_reward 2.0\n",
      "Episode finished after 270 timesteps, acc_reward 2.0\n",
      "Episode finished after 183 timesteps, acc_reward 0.0\n",
      "Episode finished after 206 timesteps, acc_reward 1.0\n",
      "Episode finished after 304 timesteps, acc_reward 2.0\n",
      "Episode finished after 339 timesteps, acc_reward 3.0\n",
      "Episode finished after 296 timesteps, acc_reward 3.0\n",
      "Episode finished after 207 timesteps, acc_reward 0.0\n",
      "Episode finished after 348 timesteps, acc_reward 3.0\n",
      "Episode finished after 218 timesteps, acc_reward 1.0\n",
      "Episode finished after 272 timesteps, acc_reward 2.0\n",
      "Episode finished after 332 timesteps, acc_reward 3.0\n",
      "Episode finished after 252 timesteps, acc_reward 1.0\n",
      "Episode finished after 241 timesteps, acc_reward 2.0\n",
      "Episode finished after 171 timesteps, acc_reward 0.0\n",
      "Episode finished after 298 timesteps, acc_reward 3.0\n",
      "Episode finished after 180 timesteps, acc_reward 0.0\n",
      "Episode finished after 243 timesteps, acc_reward 1.0\n",
      "Episode finished after 334 timesteps, acc_reward 2.0\n",
      "Episode finished after 266 timesteps, acc_reward 2.0\n",
      "Episode finished after 411 timesteps, acc_reward 4.0\n",
      "Episode finished after 193 timesteps, acc_reward 0.0\n",
      "Episode finished after 186 timesteps, acc_reward 0.0\n",
      "Episode finished after 262 timesteps, acc_reward 2.0\n",
      "Episode finished after 296 timesteps, acc_reward 2.0\n",
      "Episode finished after 242 timesteps, acc_reward 1.0\n",
      "Episode finished after 208 timesteps, acc_reward 0.0\n",
      "Episode finished after 191 timesteps, acc_reward 0.0\n",
      "Episode finished after 311 timesteps, acc_reward 2.0\n",
      "Episode finished after 366 timesteps, acc_reward 3.0\n",
      "Episode finished after 279 timesteps, acc_reward 2.0\n",
      "Episode finished after 191 timesteps, acc_reward 0.0\n",
      "Episode finished after 167 timesteps, acc_reward 0.0\n",
      "Episode finished after 159 timesteps, acc_reward 0.0\n",
      "Episode finished after 182 timesteps, acc_reward 0.0\n",
      "Episode finished after 178 timesteps, acc_reward 0.0\n",
      "Episode finished after 269 timesteps, acc_reward 1.0\n",
      "Episode finished after 215 timesteps, acc_reward 1.0\n",
      "Episode finished after 190 timesteps, acc_reward 0.0\n",
      "Episode finished after 260 timesteps, acc_reward 2.0\n",
      "Episode finished after 229 timesteps, acc_reward 1.0\n",
      "Episode finished after 173 timesteps, acc_reward 0.0\n",
      "Episode finished after 257 timesteps, acc_reward 1.0\n",
      "Episode finished after 183 timesteps, acc_reward 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 190 timesteps, acc_reward 0.0\n",
      "Episode finished after 250 timesteps, acc_reward 1.0\n",
      "Episode finished after 434 timesteps, acc_reward 4.0\n",
      "Episode finished after 182 timesteps, acc_reward 0.0\n",
      "Episode finished after 218 timesteps, acc_reward 1.0\n",
      "Episode finished after 203 timesteps, acc_reward 1.0\n",
      "Episode finished after 190 timesteps, acc_reward 0.0\n",
      "Episode finished after 202 timesteps, acc_reward 0.0\n",
      "Episode finished after 182 timesteps, acc_reward 0.0\n",
      "Episode finished after 192 timesteps, acc_reward 0.0\n",
      "Episode finished after 186 timesteps, acc_reward 0.0\n",
      "Episode finished after 243 timesteps, acc_reward 1.0\n",
      "Episode finished after 175 timesteps, acc_reward 0.0\n",
      "Episode finished after 173 timesteps, acc_reward 0.0\n",
      "Episode finished after 303 timesteps, acc_reward 2.0\n",
      "Episode finished after 241 timesteps, acc_reward 1.0\n",
      "Episode finished after 241 timesteps, acc_reward 1.0\n",
      "Episode finished after 197 timesteps, acc_reward 1.0\n",
      "Episode finished after 424 timesteps, acc_reward 4.0\n",
      "Episode finished after 216 timesteps, acc_reward 1.0\n",
      "Episode finished after 189 timesteps, acc_reward 0.0\n",
      "Episode finished after 283 timesteps, acc_reward 2.0\n",
      "Episode finished after 324 timesteps, acc_reward 2.0\n",
      "Episode finished after 259 timesteps, acc_reward 1.0\n",
      "Episode finished after 383 timesteps, acc_reward 4.0\n",
      "Episode finished after 406 timesteps, acc_reward 4.0\n",
      "Episode finished after 165 timesteps, acc_reward 0.0\n",
      "Episode finished after 176 timesteps, acc_reward 0.0\n",
      "Episode finished after 178 timesteps, acc_reward 0.0\n",
      "Episode finished after 211 timesteps, acc_reward 1.0\n",
      "Episode finished after 236 timesteps, acc_reward 1.0\n",
      "Episode finished after 180 timesteps, acc_reward 0.0\n",
      "Episode finished after 194 timesteps, acc_reward 0.0\n",
      "Episode finished after 332 timesteps, acc_reward 3.0\n",
      "Episode finished after 338 timesteps, acc_reward 2.0\n",
      "Episode finished after 162 timesteps, acc_reward 0.0\n",
      "Episode finished after 273 timesteps, acc_reward 2.0\n",
      "Episode finished after 216 timesteps, acc_reward 1.0\n",
      "Episode finished after 369 timesteps, acc_reward 3.0\n",
      "Episode finished after 202 timesteps, acc_reward 0.0\n",
      "Episode finished after 245 timesteps, acc_reward 1.0\n",
      "Episode finished after 218 timesteps, acc_reward 1.0\n",
      "Episode finished after 261 timesteps, acc_reward 1.0\n",
      "Episode finished after 240 timesteps, acc_reward 1.0\n",
      "Episode finished after 382 timesteps, acc_reward 3.0\n",
      "Episode finished after 392 timesteps, acc_reward 4.0\n",
      "Episode finished after 315 timesteps, acc_reward 2.0\n",
      "Episode finished after 308 timesteps, acc_reward 2.0\n",
      "Episode finished after 239 timesteps, acc_reward 1.0\n",
      "Episode finished after 169 timesteps, acc_reward 0.0\n",
      "Episode finished after 183 timesteps, acc_reward 0.0\n",
      "Episode finished after 170 timesteps, acc_reward 0.0\n",
      "Episode finished after 175 timesteps, acc_reward 0.0\n",
      "Episode finished after 183 timesteps, acc_reward 0.0\n",
      "Episode finished after 200 timesteps, acc_reward 0.0\n",
      "Episode finished after 307 timesteps, acc_reward 3.0\n",
      "Episode finished after 282 timesteps, acc_reward 2.0\n",
      "Episode finished after 172 timesteps, acc_reward 0.0\n",
      "Episode finished after 328 timesteps, acc_reward 3.0\n",
      "Episode finished after 503 timesteps, acc_reward 6.0\n",
      "Episode finished after 323 timesteps, acc_reward 3.0\n",
      "Episode finished after 226 timesteps, acc_reward 1.0\n",
      "Episode finished after 243 timesteps, acc_reward 1.0\n",
      "Episode finished after 269 timesteps, acc_reward 2.0\n",
      "Episode finished after 184 timesteps, acc_reward 0.0\n",
      "Episode finished after 310 timesteps, acc_reward 3.0\n",
      "Episode finished after 335 timesteps, acc_reward 3.0\n",
      "Episode finished after 406 timesteps, acc_reward 4.0\n",
      "Episode finished after 372 timesteps, acc_reward 4.0\n",
      "Episode finished after 270 timesteps, acc_reward 2.0\n",
      "Episode finished after 167 timesteps, acc_reward 0.0\n",
      "Episode finished after 340 timesteps, acc_reward 3.0\n",
      "Episode finished after 183 timesteps, acc_reward 0.0\n",
      "Episode finished after 218 timesteps, acc_reward 1.0\n",
      "Episode finished after 289 timesteps, acc_reward 1.0\n",
      "Episode finished after 250 timesteps, acc_reward 1.0\n",
      "Episode finished after 242 timesteps, acc_reward 1.0\n",
      "Episode finished after 351 timesteps, acc_reward 3.0\n",
      "Episode finished after 226 timesteps, acc_reward 1.0\n",
      "Episode finished after 299 timesteps, acc_reward 2.0\n",
      "Episode finished after 227 timesteps, acc_reward 1.0\n",
      "Episode finished after 163 timesteps, acc_reward 0.0\n",
      "Episode finished after 292 timesteps, acc_reward 2.0\n",
      "Episode finished after 337 timesteps, acc_reward 4.0\n",
      "Episode finished after 344 timesteps, acc_reward 3.0\n",
      "Episode finished after 230 timesteps, acc_reward 1.0\n",
      "Episode finished after 224 timesteps, acc_reward 1.0\n",
      "Episode finished after 242 timesteps, acc_reward 1.0\n",
      "Episode finished after 203 timesteps, acc_reward 1.0\n",
      "Episode finished after 291 timesteps, acc_reward 2.0\n",
      "Episode finished after 350 timesteps, acc_reward 3.0\n",
      "Episode finished after 176 timesteps, acc_reward 0.0\n",
      "Episode finished after 168 timesteps, acc_reward 0.0\n",
      "Episode finished after 200 timesteps, acc_reward 1.0\n",
      "Episode finished after 277 timesteps, acc_reward 2.0\n",
      "Episode finished after 244 timesteps, acc_reward 1.0\n",
      "Episode finished after 309 timesteps, acc_reward 3.0\n",
      "Episode finished after 291 timesteps, acc_reward 2.0\n",
      "Episode finished after 208 timesteps, acc_reward 1.0\n",
      "Episode finished after 173 timesteps, acc_reward 0.0\n",
      "Episode finished after 158 timesteps, acc_reward 0.0\n",
      "Episode finished after 399 timesteps, acc_reward 4.0\n",
      "Episode finished after 279 timesteps, acc_reward 2.0\n",
      "Episode finished after 326 timesteps, acc_reward 3.0\n",
      "Episode finished after 182 timesteps, acc_reward 0.0\n",
      "Episode finished after 288 timesteps, acc_reward 2.0\n",
      "Episode finished after 189 timesteps, acc_reward 0.0\n",
      "Episode finished after 178 timesteps, acc_reward 0.0\n",
      "Episode finished after 497 timesteps, acc_reward 6.0\n",
      "Episode finished after 340 timesteps, acc_reward 3.0\n",
      "Episode finished after 353 timesteps, acc_reward 4.0\n",
      "Episode finished after 263 timesteps, acc_reward 2.0\n",
      "Episode finished after 277 timesteps, acc_reward 2.0\n",
      "Episode finished after 174 timesteps, acc_reward 0.0\n",
      "Episode finished after 197 timesteps, acc_reward 0.0\n",
      "Episode finished after 199 timesteps, acc_reward 0.0\n",
      "Episode finished after 277 timesteps, acc_reward 2.0\n",
      "Episode finished after 260 timesteps, acc_reward 1.0\n",
      "Episode finished after 310 timesteps, acc_reward 2.0\n",
      "Episode finished after 163 timesteps, acc_reward 0.0\n",
      "Episode finished after 264 timesteps, acc_reward 2.0\n",
      "Episode finished after 195 timesteps, acc_reward 0.0\n",
      "Episode finished after 247 timesteps, acc_reward 2.0\n",
      "Episode finished after 204 timesteps, acc_reward 1.0\n",
      "Episode finished after 192 timesteps, acc_reward 0.0\n",
      "Episode finished after 226 timesteps, acc_reward 1.0\n",
      "Episode finished after 302 timesteps, acc_reward 2.0\n",
      "Episode finished after 275 timesteps, acc_reward 2.0\n",
      "Episode finished after 274 timesteps, acc_reward 2.0\n",
      "Episode finished after 179 timesteps, acc_reward 0.0\n",
      "Episode finished after 259 timesteps, acc_reward 2.0\n",
      "Episode finished after 346 timesteps, acc_reward 3.0\n",
      "Episode finished after 169 timesteps, acc_reward 0.0\n",
      "Episode finished after 415 timesteps, acc_reward 4.0\n",
      "Episode finished after 362 timesteps, acc_reward 3.0\n",
      "Episode finished after 282 timesteps, acc_reward 2.0\n",
      "Episode finished after 223 timesteps, acc_reward 1.0\n",
      "Episode finished after 300 timesteps, acc_reward 2.0\n",
      "Episode finished after 225 timesteps, acc_reward 1.0\n",
      "Episode finished after 244 timesteps, acc_reward 1.0\n",
      "Episode finished after 210 timesteps, acc_reward 1.0\n",
      "Episode finished after 173 timesteps, acc_reward 0.0\n",
      "Episode finished after 249 timesteps, acc_reward 1.0\n",
      "Episode finished after 224 timesteps, acc_reward 1.0\n",
      "Episode finished after 311 timesteps, acc_reward 3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [conv_layer(3, 16, stride=2), conv_layer(16, 32, stride=2), conv_layer(32, 128, stride=2), \n",
    "          nn.AvgPool2d(4), Flatten()]\n",
    "cnn = nn.Sequential(*layers)\n",
    "\n",
    "q_mod = Q_model(cnn, 3840, 4)\n",
    "q_mod_target = Q_model(cnn, 3840, 4)\n",
    "q_mod_target.load_state_dict(q_mod.state_dict())\n",
    "\n",
    "it_bf = 50 #iterations before updating the target network\n",
    "q_mod.cuda()\n",
    "q_mod_target.cuda()\n",
    "\n",
    "opt = torch.optim.Adam(q_mod.parameters())\n",
    "\n",
    "discount = 0.1\n",
    "\n",
    "replay_mem = ReplayMemory(capacity=1000)\n",
    "\n",
    "rewards = []\n",
    "\n",
    "#times = []\n",
    "batch_size = 64\n",
    "\n",
    "count = 0\n",
    "p_init = 0.5 # init probability of exploration\n",
    "p_final = 0.01 # final probability of exploration\n",
    "decay = 0.001\n",
    "\n",
    "for i_episode in tqdm_notebook(range(300)):\n",
    "    last_observation = tensor(env.reset(), dtype=torch.float32).view(3, 160, -1).cuda()\n",
    "    acc_reward = 0\n",
    "    theta = 0.1\n",
    "    total_iterations = 1000\n",
    "    \n",
    "    for t in range(total_iterations):\n",
    "        \n",
    "        if t == 0 :\n",
    "            observation, reward, done, info = env.step(env.action_space.sample())\n",
    "            observation = tensor(observation, dtype=torch.float32).view(3, 160, -1).cuda()\n",
    "    \n",
    "        count += 1\n",
    "        if count % it_bf :\n",
    "            q_mod_target.load_state_dict(q_mod.state_dict())\n",
    "        \n",
    "        env.render()\n",
    "\n",
    "        prob_exploration = p_init + (p_final - p_init) * math.exp(-1 * decay * (i_episode+1) * t) #prob of exploration\n",
    "        \n",
    "        if np.random.rand() < prob_exploration :\n",
    "            action = tensor([env.action_space.sample()]).cuda()\n",
    "        else :\n",
    "            inp = (last_observation.unsqueeze(0).cuda(), observation.unsqueeze(0).cuda())\n",
    "            action = q_mod(*inp).argmax(1).cuda()\n",
    "\n",
    "\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        new_observation = tensor(new_observation, dtype=torch.float32).view(3, 160, -1).cuda()\n",
    "        \n",
    "\n",
    "        replay_mem.push((last_observation, observation), action, (observation, new_observation), tensor([[reward]]).cuda())\n",
    "        \n",
    "        #training\n",
    "        transitions = replay_mem.sample(min(batch_size, len(replay_mem)))\n",
    "        model_input = (torch.cat([frame[0][0].unsqueeze(0) for frame in transitions]),\n",
    "                       torch.cat([frame[0][1].unsqueeze(0) for frame in transitions]))\n",
    "        action_batch = torch.cat([a[1] for a in transitions]).unsqueeze(1)\n",
    "        reward_batch = torch.cat([a[3] for a in transitions]).cuda()\n",
    "        model_target_input = (torch.cat([frame[2][0].unsqueeze(0) for frame in transitions]),\n",
    "                               torch.cat([frame[2][1].unsqueeze(0) for frame in transitions]))\n",
    "        preds = q_mod(*model_input).gather(1, action_batch)\n",
    "        targets = q_mod_target(*model_target_input).max(1)[0].detach().unsqueeze(1)\n",
    "        targets = targets* 0.1  + reward_batch\n",
    "        loss = F.smooth_l1_loss(preds, targets)\n",
    "\n",
    "        #time_b = time.time()\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        #time_a= time.time()\n",
    "\n",
    "        #times.append(time_a-time_b)\n",
    "        for param in q_mod.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        opt.step()\n",
    "        \n",
    "        acc_reward += reward\n",
    "        \n",
    "        last_observation = observation\n",
    "        observation = new_observation\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps, acc_reward {}\".format(t+1, acc_reward))\n",
    "            rewards.append(acc_reward)\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
